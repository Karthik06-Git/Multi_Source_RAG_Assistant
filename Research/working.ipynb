{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3624f9",
   "metadata": {},
   "source": [
    "## **LLM-Powered Multi-URL Information Retrival Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd920fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0a4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables and Set API Keys\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(urls: List[Optional[str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Given a list of URLs, process these URLs to create a vector store using HuggingFace-Embeddings and FAISS-DB.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = UnstructuredURLLoader(urls=urls)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators = [\n",
    "                \"\\n### \",     # Markdown-style or section headers\n",
    "                \"\\n## \",\n",
    "                \"\\n# \",\n",
    "                \"\\n\\n\",       # Paragraphs\n",
    "                \"\\n\",         # Newlines\n",
    "            ],\n",
    "            chunk_size=1000, chunk_overlap=200\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vector_store = FAISS.from_documents(\n",
    "            documents = doc_splits,\n",
    "            embedding = hf_embeddings\n",
    "        )\n",
    "\n",
    "        with open(\"VectorStoreDB/faiss_vectorstore.joblib\", \"wb\") as index_file:\n",
    "            joblib.dump(vector_store, index_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating vectorstore: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_pipeline(get_session_history: callable):\n",
    "    \"\"\"\n",
    "    Create a retrieval chain using the vector store.\n",
    "    \"\"\"\n",
    "    if os.path.exists(\"VectorStoreDB/faiss_vectorstore.joblib\"):\n",
    "        with open(\"VectorStoreDB/faiss_vectorstore.joblib\", \"rb\") as file:\n",
    "            vector_store = joblib.load(file)\n",
    "\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    ## LLM-model\n",
    "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\", groq_api_key=groq_api_key)\n",
    "\n",
    "\n",
    "    contextualize_q_prompt = \"\"\"\n",
    "            Given the chat history and the latest user question, reformulate the current question into a standalone version that can be understood without the previous context.\n",
    "\n",
    "            Use the chat history only to clarify references (like “it”, “this function”, or “that library”).\n",
    "            Do not answer the question.\n",
    "\n",
    "            ---\n",
    "            ### Chat History:\n",
    "            {chat_history}\n",
    "            ---\n",
    "            ### Latest User Question:\n",
    "            {input}\n",
    "            ---\n",
    "\n",
    "            ### Reformulated Standalone Question\n",
    "\n",
    "    \"\"\"\n",
    "    contextualize_q_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_with_history_prompt)\n",
    "\n",
    "    qa_system_prompt = \"\"\"\n",
    "            You are an advanced AI documentation assistant designed to help users by answering queries using \n",
    "            the provided context from multiple documentation sources.\n",
    "\n",
    "            Your primary goal is to deliver technically accurate, concise, and complete answers **strictly grounded in the retrieved context** and chat history.\n",
    "            Do not hallucinate, assume, or fabricate information that is not supported by the given documentation.\n",
    "\n",
    "            ---\n",
    "            ### Context Information:\n",
    "            {context}\n",
    "            ---\n",
    "            ### Chat History:\n",
    "            {chat_history}\n",
    "            ---\n",
    "            ### User Query:\n",
    "            {input}\n",
    "            ---\n",
    "\n",
    "            ### Response Guidelines:\n",
    "            1. **Grounding:** Use only the information provided in the context to answer the question.  \n",
    "            2. **Missing Info:** If the answer cannot be fully determined from the context, respond exactly with:  \n",
    "            > \"The provided documentation does not contain enough information to answer that precisely.\"\n",
    "            3. **Clarity:** Write in a clear, developer-friendly tone. Avoid unnecessary repetition or overly generic statements.  \n",
    "            4. **Structure:**  \n",
    "                - Use Markdown formatting.  \n",
    "                - Include **headings**, **bullet points**, and **code blocks** where appropriate.  \n",
    "                - When multiple documents support the answer, synthesize them into a cohesive explanation.  \n",
    "            5. **Source Attribution (if available):** At the end, list document names or URLs from which the information was derived.  \n",
    "            6. **Consistency:** Maintain conversation context and continuity with previous answers (from chat history).  \n",
    "            7. **Style:** Be factual, concise, and instructional — as if you were a senior developer or API mentor.\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### Response Format:\n",
    "            **Answer:**\n",
    "            (Provide a clear explanation or step-by-step guide.)\n",
    "\n",
    "            **Example (if applicable):**\n",
    "            ```python\n",
    "            # relevant code sample or command\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    qa_system_prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    qa_chain = qa_system_prompt_template | llm | parser\n",
    "\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        history_aware_retriever,\n",
    "        qa_chain\n",
    "    )\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\"\n",
    "    )\n",
    "\n",
    "    return conversational_rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e290e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastAPI app\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "        title=\"LLM-Powered Multi-URL RAG Chatbot\",\n",
    "        version=\"1.0\",\n",
    "        description=\"A RAG chatbot using multiple URLs as context\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b78bfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"RAG Chatbot Backend is running successfully!\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class URLsRequest(BaseModel):\n",
    "    urls : List[Optional[str]]\n",
    "\n",
    "\n",
    "# API Endpoint to process URLs and create vectorstore\n",
    "@app.post(\"/process_urls\")\n",
    "async def process_urls(request: URLsRequest):\n",
    "    \"\"\"\n",
    "    API endpoint to process a list of URLs and create a vectorstore.\n",
    "    \"\"\"\n",
    "    urls = request.urls\n",
    "    is_created = create_vectorstore(urls)\n",
    "    if is_created:\n",
    "        return {\"status\": True,\n",
    "                \"message\": \"Vectorstore created successfully from the provided URLs.\"}\n",
    "    else:\n",
    "        return {\"status\": False,\n",
    "                \"message\": \"Failed to create vectorstore. Please check the URLs and try again.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac333b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatResponseRequest(BaseModel):\n",
    "    session_id : str\n",
    "    user_query : str\n",
    "\n",
    "\n",
    "# API Endpoint to get chat response   \n",
    "@app.post(\"/chat_response\")\n",
    "async def chat_response(request: ChatResponseRequest):\n",
    "    \"\"\"\n",
    "    API endpoint to get a chat response for a given user query and session ID.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        \"\"\"Get the chat message history for a given session ID.\"\"\"\n",
    "        if not hasattr(app.state, \"store\"):\n",
    "            app.state.store = {}\n",
    "        if session_id not in app.state.store:\n",
    "            app.state.store[session_id] = ChatMessageHistory()\n",
    "        return app.state.store[session_id]\n",
    "    \n",
    "    # Extracting session_id and user_query from the JSON-request\n",
    "    session_id = request.session_id\n",
    "    user_query = request.user_query\n",
    "\n",
    "    rag_chain = create_rag_pipeline(\n",
    "        get_session_history=get_session_history\n",
    "    )\n",
    "\n",
    "    response = rag_chain.invoke(\n",
    "        {\"input\": user_query},\n",
    "        config = {\n",
    "            \"configurable\": {\"session_id\": session_id}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ddc76f",
   "metadata": {},
   "source": [
    "#### To Run FastAPI server in the Notebook \n",
    "We use `Thread`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4560]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:54288 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:54288 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:64418 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:64418 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karth\\Krish_Naik\\Complete_GenAI_using_Langchain_and_HuggingFace\\GenAI_LLMs\\Langchain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60956 - \"POST /process_urls HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60987 - \"POST /chat_response?session_id=abc123&user_query=What%20is%20an%20Array%20in%20Numpy%20%3F HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:56516 - \"POST /chat_response?session_id=abc123&user_query=Why%20to%20use%20Numpy%20and%20give%20me%20its%20applications%20%3F%3F HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55028 - \"POST /chat_response?session_id=abc123&user_query=What%20are%20the%20previous%20questions%20i%20asked%20about%20numpy%20%3F HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:53526 - \"POST /chat_response?session_id=abc123&user_query=Explain%20the%20Series%20Data%20structure%20in%20pandas HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:51537 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:51537 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from threading import Thread\n",
    "\n",
    "# Allow nested event loops (Jupyter already runs one)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def run_app():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n",
    "# Start the FastAPI server\n",
    "thread = Thread(target=run_app, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c9791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb9b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
